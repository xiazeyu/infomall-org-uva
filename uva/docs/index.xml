<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Infomall.org – Content</title>
    <link>https://infomall.org/uva/docs/</link>
    <description>Recent content in Content on Infomall.org</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://infomall.org/uva/docs/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Geoffrey C. Fox</title>
      <link>https://infomall.org/uva/docs/staff/fox/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/staff/fox/</guid>
      <description>
        
        
        
        <![CDATA[<img src="https://infomall.org/uva/docs/staff/fox/featured-fox_hu4da4c533d4f8001d1c59cf6f71272919_896203_640x0_resize_catmullrom_3.png" width="640" height="853"/>]]>
        
        






&lt;figure class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 210px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://infomall.org/uva/uva/docs/staff/fox/featured-fox_hu4da4c533d4f8001d1c59cf6f71272919_896203_200x300_fill_catmullrom_smart1_3.png&#34; width=&#34;200&#34; height=&#34;300&#34;&gt;
	
	&lt;figcaption class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Geoffrey C. Fox
&lt;/p&gt;
	&lt;/figcaption&gt;
	
&lt;/figure&gt;



&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Geoffrey C. Fox.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;The Web Page is located at&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://biocomplexity.virginia.edu/person/geoffrey-fox&#34;&gt;https://biocomplexity.virginia.edu/person/geoffrey-fox&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Cylon</title>
      <link>https://infomall.org/uva/docs/projects/cylon/</link>
      <pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/cylon/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;One of the most exciting aspects of the Big Data era for both the industry and research communities is the incredible progress being made in the domains of the machine and deep learning. Modern applications demand resources that are more than a single node can supply.
The difficulties that the total data processing environment must address include a variety of data engineering for pre- and post-data processing, communication, and system integration. The ability of data analytics tools to quickly interface with existing frameworks in a variety of languages is a crucial requirement as it increases user productivity and efficiency.All of this calls for an effective and widely dispersed integrated approach to data processing, yet many of today&amp;rsquo;s well-liked data analytics solutions are unable to simultaneously meet all of these criteria.&lt;/p&gt;
&lt;p&gt;In this project, we introduce Cylon, a high-performance distributed data processing toolkit that is open-source and easily integrated with current Big Data and AI/ML frameworks. It has a compact data structure as the foundation, a versatile C++ core, and language bindings for Python, Java, and C++ on top of it.&lt;/p&gt;
&lt;p&gt;We develop Cylon&amp;rsquo;s design and demonstrate how it can be used as a standalone framework or imported as a library into already-existing applications. Early tests reveal that Cylon boosts well-known technologies like Apache Spark and Dask with significant performance gains for crucial operations and improved component linkages. The ultimate goal is to demonstrate how Cylon&amp;rsquo;s design supports cross-platform usage with the least amount of overhead, which includes well-known AI tools like PyTorch, Tensorflow, and Jupyter notebooks.&lt;/p&gt;

&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Cosmoflow</title>
      <link>https://infomall.org/uva/docs/projects/cosmoflow/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/cosmoflow/</guid>
      <description>
        
        
        &lt;h2 id=&#34;high-performance-computing-with-the-cosmoflow-benchmark&#34;&gt;High Performance Computing with the Cosmoflow Benchmark&lt;/h2&gt;
&lt;p&gt;CosmoFlow is a deep learning application that uses a convolutional neural network to predict the large-scale structure of the universe from cosmological simulations. Developed by researchers from Oak Ridge National Laboratory and NVIDIA, the project achieved a 4x speedup compared to traditional methods by leveraging GPUs and high-performance computing. CosmoFlow generates large datasets that researchers can analyze to gain insights into the evolution of the cosmos. The neural network is trained on 3D images generated by cosmological simulations, and the resulting datasets provide valuable information about the formation of galaxies, distribution of dark matter, and overall evolution of the universe. CosmoFlow enables faster and more efficient simulations, opening up new avenues of research in this area. The project has the potential to revolutionize cosmology research by enabling faster and more accurate simulations of the universe.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/DSC-SPIDAL/mlcommons-cosmoflow&#34;&gt;https://github.com/DSC-SPIDAL/mlcommons-cosmoflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mlcommons/hpc&#34;&gt;https://github.com/mlcommons/hpc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;report (TBD)&lt;/li&gt;
&lt;li&gt;paper (TBD)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MLcommons repository of cosmoflow, &lt;a href=&#34;https://github.com/mlcommons/hpc/tree/main/cosmoflow&#34;&gt;https://github.com/mlcommons/hpc/tree/main/cosmoflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DSC respository of cosmoflow, &lt;a href=&#34;https://github.com/DSC-SPIDAL/mlcommons-cosmoflow&#34;&gt;https://github.com/DSC-SPIDAL/mlcommons-cosmoflow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Cylon on AWS</title>
      <link>https://infomall.org/uva/docs/projects/cyloncloud/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/cyloncloud/</guid>
      <description>
        
        
        &lt;h2 id=&#34;high-performance-data-engineering-with-cylon-on-amazon-web-services&#34;&gt;High Performance Data Engineering with Cylon on Amazon Web Services&lt;/h2&gt;


&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;In recent years the data engineering discipline has been greatly impacted by Artificial Intelligence (AI) and Machine Learning (ML). The effect has ushered in research related to the speed, performance, and optimization of such processes [2]. To meet these ends, many frameworks have been proposed.  One such framework is CylonData [1].  CylonData represents an architecture where performance critical operations are moved to a highly optimized library. Moreover, the architecture provides the capability to leverage the performance associated with in-memory data and distributed operations and data across processes, a key requirement related to processing large data engineering workloads at scale. Such benefits are realized, for example, in the conversion from tabular or table format to tensor format required for ML/DL or via the use of relational algebraic expressions such as joins, select, project, etc. More specifically Cylon is described as &amp;ldquo;a fast, scalable distributed memory data parallel library for processing structured data&amp;rdquo; [1].  While CylonData has focused on a MPI implementation using HPC ML resources, the research work here is to port this to a serverless compute infrastructure within AWS services such as AWS Lambda, ECS, EC2, Route 53, ALB, and EFS.  Once this is completed we will be achieving two things. First, we will be showcasing this work will be available not only on HPC but also on AWS serverless and serverful compute resources.  Second, we will be able to provide an extensive benchmark comparison between HPC and Serverless/Serverful Computing to showcase the strengths and weaknesses of both approaches.&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cylondata/cylon&#34;&gt;https://github.com/cylondata/cylon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;report (TBD)&lt;/li&gt;
&lt;li&gt;paper (TBD)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1, April 2023: Merge of UCC/UCX Bootstrapping to cylondata/man&lt;/li&gt;
&lt;li&gt;25, April 2023: Execution of Serverful Cylon Delivered via ECS Infrastructure using OpenMPI&lt;/li&gt;
&lt;li&gt;10, May 2023: Prototype of Servlerless delivery of cylon library via AWS Lambda Layers&lt;/li&gt;
&lt;li&gt;30, May 2023: Serverless Cache infrastructure fascade prototype and support via abstraction in the cylon source&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Cylon.&amp;rdquo; cylondata/cylon, &lt;a href=&#34;https://github.com/cylondata/cylon/&#34;&gt;https://github.com/cylondata/cylon/&lt;/a&gt;.  Accessed 9 September 2022.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Cylon Library for Fast &amp;amp; Scalable Data Engineering.&amp;rdquo; Cylon Blog, &lt;a href=&#34;https://supun-kamburugamuve.medium.com/cylon-library-for-fast-scalable-data-engineering-bf74742fe5d1&#34;&gt;https://supun-kamburugamuve.medium.com/cylon-library-for-fast-scalable-data-engineering-bf74742fe5d1&lt;/a&gt;.  Accessed 9 September 2022.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Data Compiler</title>
      <link>https://infomall.org/uva/docs/projects/data-compiler/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/data-compiler/</guid>
      <description>
        
        
        &lt;h2 id=&#34;optimizing-large-scale-deep-learning-by-data-movement-aware-compiler&#34;&gt;Optimizing Large-Scale Deep Learning by Data Movement-Aware Compiler&lt;/h2&gt;
&lt;p&gt;Projet Members&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://luosuu.github.io/&#34;&gt;Tianle Zhong&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-summary&#34;&gt;Project Summary&lt;/h2&gt;
&lt;p&gt;This project aims to address the data movement, as a known major efficiency bottleneck of distributed training[1], by designing a tensor compiler[2] which can acquire and optimize the data movement graph and scheduling at the compilation time so that the execution becomes fully static for higher performance. Such Ahead-of-Time(AOT) optimization also enables opportunities for auto-parallelism and pipelining like [3, 5]. The current exploration is about leveraging Multi-Level Intermediate Representation (MLIR)[4] to include data movement information into the compilation passes.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Proposal: WIP&lt;/li&gt;
&lt;li&gt;GitHub repo: not ready for open access&lt;/li&gt;
&lt;li&gt;Report: TBD&lt;/li&gt;
&lt;li&gt;Paper: TBD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;15 April 2023: Define a MLIR dialect which can describe data movement of deep learning&lt;/li&gt;
&lt;li&gt;1 June 2023: Figure out the optimization over the dialect converting and lowering passes&lt;/li&gt;
&lt;li&gt;15 July 2023: Prototype the backend code generation for real-world tests&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Ivanov, Andrei, et al. &amp;ldquo;Data movement is all you need: A case study on optimizing transformers.&amp;rdquo; &lt;em&gt;Proceedings of Machine Learning and Systems&lt;/em&gt; 3 (2021): 711-732. available at &lt;a href=&#34;https://arxiv.org/abs/2007.00072&#34;&gt;https://arxiv.org/abs/2007.00072&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kjolstad, Fredrik, et al. &amp;ldquo;The tensor algebra compiler.&amp;rdquo; &lt;em&gt;Proceedings of the ACM on Programming Languages&lt;/em&gt; 1.OOPSLA (2017): 1-29. available at &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3133901&#34;&gt;https://dl.acm.org/doi/10.1145/3133901&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Yuan, Jinhui, et al. &amp;ldquo;Oneflow: Redesign the distributed deep learning framework from scratch.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2110.15032&#34;&gt;https://arxiv.org/abs/2110.15032&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vasilache, Nicolas, et al. &amp;ldquo;Composable and modular code generation in MLIR: A structured and retargetable approach to tensor compiler construction.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2202.03293&#34;&gt;https://arxiv.org/abs/2202.03293&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zheng, Lianmin, et al. &amp;ldquo;Alpa: Automating Inter-and {Intra-Operator} Parallelism for Distributed Deep Learning.&amp;rdquo; &lt;em&gt;16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)&lt;/em&gt;. available at &lt;a href=&#34;https://arxiv.org/abs/2201.12023&#34;&gt;https://arxiv.org/abs/2201.12023&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: NIST</title>
      <link>https://infomall.org/uva/docs/projects/nist/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/nist/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Over the last several years, the computation landscape for conducting
data an- alytics has completely changed. While in the past a lot of
the activities have been undertaken in isolation by companies and
research institutions, today’s in- frastructure constitutes a wealth
of services offered by a variety of providers that offer opportunities
for reuse and interactions.&lt;/p&gt;
&lt;p&gt;We will expand analytics services to focus on developing a frame- work
for reusable hybrid multi-service data analytics. It includes (a) a
technology review that explicitly targets the intersection of hybrid
multi-provider analytics services (b) enhancing the concepts of
services to showcase how hybrid, as well as multi-provider services,
can be integrated and reused via the proposed framework, (d) address
analytics service composition, and (c) integrate container
technologies to achieve state-of-the-art analytics service deployment
capabilities.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;PI: Gregor von Laszewski, &lt;a href=&#34;mailto:laszewski@gmail.com&#34;&gt;laszewski@gmail.com&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: OSMI</title>
      <link>https://infomall.org/uva/docs/projects/osmi/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/projects/osmi/</guid>
      <description>
        
        
        &lt;h2 id=&#34;mlcommons-osmi-benchmark&#34;&gt;MLCommons OSMI Benchmark&lt;/h2&gt;
&lt;p&gt;OSMI-Bench explores the optimal deployment of machine-learned surrogate (MLS) models in rotorcraft aerodynamics on high-performance computers (HPC). In this benchmark, we test three rotorcraft models for optimal deployment configurations, including, Long Short Term Memory (LSTM), Convolutional Neural Network (CNN), and Temporal Convolutional Neural Network (TCNN) models with 2M, 44M, and 212M trainable parameters respectively [1]. Surrogate models trained on synthetic data were selected because we are solely focused on inference efficiency not model accuracy. We are now running the benchmark on the Rivanna HPC at the University of Virginia to find the optimal deployment scenario for each model, and we plan to develop more models to benchmark, such as a transformer-encoder natural language model. We are also investigating the relationship between batchsize, GPU, and concurrency and inference throughput/time. We will soon explore running the load balancers used in the OSMI-Bench framework, such as Python concurrent.futures threadpool, HAProxy and mpi4py, on Rivanna.&lt;/p&gt;
&lt;h2 id=&#34;deliverables&#34;&gt;Deliverables&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;gitrepo[https://github.com/laszewsk/osmi]&lt;/li&gt;
&lt;li&gt;report (TBD)&lt;/li&gt;
&lt;li&gt;paper (TBD)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;31 March 2023: Graph the relationship between configuration and inference performance for each model, complete OSMI-Bench documentation for Rivanna&lt;/li&gt;
&lt;li&gt;15 April: Run load balancer on Rivanna&lt;/li&gt;
&lt;li&gt;1 May 2023: Develop and benchmark new models&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Wesley Brewer et al. &amp;ldquo;Production Deployment of Machine-Learned Rotorcraft Surrogate Models on HPC&amp;rdquo;, 2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC), 15 November 2021, 10.1109/MLHPC54614.2021.00008, [https://ieeexplore.ieee.org/document/9652868]&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Gregor von Laszewski</title>
      <link>https://infomall.org/uva/docs/staff/vonlaszewski/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/staff/vonlaszewski/</guid>
      <description>
        
        
        
        <![CDATA[<img src="https://infomall.org/uva/docs/staff/vonlaszewski/featured-gregor_hufb2c7820c0f3f4045b6bcda1b9cc9f4a_153096_640x0_resize_q75_catmullrom.jpg" width="640" height="853"/>]]>
        
        






&lt;figure class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 310px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://infomall.org/uva/uva/docs/staff/vonlaszewski/featured-gregor_hufb2c7820c0f3f4045b6bcda1b9cc9f4a_153096_300x400_fill_q75_catmullrom_smart1.jpg&#34; width=&#34;300&#34; height=&#34;400&#34;&gt;
	
	&lt;figcaption class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Gregor von Laszewski
&lt;/p&gt;
	&lt;/figcaption&gt;
	
&lt;/figure&gt;



&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Gregor von Laszewski&amp;rsquo;s Web page is located at&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://laszewski.github.io/&#34;&gt;https://laszewski.github.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Nate Kimball</title>
      <link>https://infomall.org/uva/docs/staff/kimball/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/staff/kimball/</guid>
      <description>
        
        
        
        <![CDATA[<img src="https://infomall.org/uva/docs/staff/kimball/featured-kimball_hu6a9f2f962359f1f1c5499d6cdc955f3a_860336_640x0_resize_q75_catmullrom.jpg" width="640" height="851"/>]]>
        
        &lt;hr&gt;







&lt;figure class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 210px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://infomall.org/uva/uva/docs/staff/kimball/featured-kimball_hu6a9f2f962359f1f1c5499d6cdc955f3a_860336_200x300_fill_q75_catmullrom_smart1.jpg&#34; width=&#34;200&#34; height=&#34;300&#34;&gt;
	
	&lt;figcaption class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Nate Kimball
&lt;/p&gt;
	&lt;/figcaption&gt;
	
&lt;/figure&gt;



&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Nathaniel Kimball is a student at the University of Virginia, class of
2024, pursuing a B.S. in Computer Science with a minor in Applied
Mathematics. He is an incoming Global Technology Summer Analyst intern
at Bank of America (2023). His interests include artificial
intelligence, machine learning, MLOps, stochastic processes, and
algorithmic economics.&lt;/p&gt;

&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Varun Pavuloori</title>
      <link>https://infomall.org/uva/docs/staff/pavuloori/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/staff/pavuloori/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Varun Pavuloori&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Varun Pavuloori is an undergraduate student at the University of
Virginia, class of 2025, pursuing a B.S. in Computer Science and a
minor in Data Science.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Arup Sarker</title>
      <link>https://infomall.org/uva/docs/staff/sarker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/staff/sarker/</guid>
      <description>
        
        
        &lt;hr&gt;







&lt;figure class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 210px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://infomall.org/uva/uva/docs/staff/sarker/arup_hu1242976d708648dbed6be45da573f9d5_304640_200x300_fill_q75_catmullrom_smart1.jpg&#34; width=&#34;200&#34; height=&#34;300&#34;&gt;
	
	&lt;figcaption class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Arup Sarker
&lt;/p&gt;
	&lt;/figcaption&gt;
	
&lt;/figure&gt;



&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Arup Sarker is a student at the University of Virginia, class of
2021, pursuing a Ph.D. in Computer Science. He is working as a research assistant at the UVA biocomplexity Institute where he currently focuses on developing an HPC framework for a distributed environment. In the past, until the middle of last year, he worked on 3D data processing based on point cloud or voxel-based motion planning and control in an autonomous vehicle, as well as security research in IoT Trust computing. Prior to this, he was a Staff Engineer at Samsung R&amp;amp;D Institute Bangladesh Ltd. starting from September 2011. During his tenure, he had the opportunity to work with various cutting-edge technologies that were leading the market. He made significant contributions to the development of the Android platform, particularly on Audio/Video, Frameworks, and Network protocols for transmitting data in mission-critical services, as well as Gear360 and many others. From the beginning of his time at Samsung, all of his work revolved around designing and developing embedded and distributed solutions that required data transmission over networks. Even as a feature phone developer in 2011, he was responsible for the system modules used to transmit calls and messages.&lt;/p&gt;
&lt;p&gt;His interests include High-Performance Computing in Distributed Environments, Multi-Level Compiler optimization for parallel programming, Distributed heterogeneous AI architecture, and 3D vision for the autonomous system.&lt;/p&gt;
&lt;p&gt;Checkout the detailed profile: &lt;a href=&#34;https://www.arupsarker.com/&#34;&gt;https://www.arupsarker.com/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: Mills W. Staylor</title>
      <link>https://infomall.org/uva/docs/staff/staylor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/staff/staylor/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Mills W. Staylor&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Mills W. Staylor received his BS in Business Management from Virginia
Polytechnic Institute and State University (Virginia Tech).  He
continued his studies at George Mason University where he earned dual
MS degrees in Civil, Environmental and Infrastructural Engineering and
Software Engineering.  After graduate school, Mills worked as a
professional software engineer in both public and private sectors.
Most recently, he played a pivotal role in the development of an Alexa
bridge which provided access to streaming content on Alexa devices.
He is currently a PhD student in Computer Science at the University of
Virginia and has interests in high performance computing,
visualization, and cloud computing.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Tianle Zhong</title>
      <link>https://infomall.org/uva/docs/staff/zhong/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/staff/zhong/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;Tianle Zhong&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Tianle Zhong received his B.Eng. in Computer Science and B.S. in
Applied Mathematics at the University of Electronic Science and
Technology of China in 2022. He is currently working for his Ph.D. in
Computer Science at University of Virginia. His research interests
include machine learning system and deep learning compiler,
specifically about the optimization and efficiency of large model
training.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Links</title>
      <link>https://infomall.org/uva/docs/publications/links/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/publications/links/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;This page includes the links to the papers and presentations.&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&#34;publications&#34;&gt;Publications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/publications&#34;&gt;Publications&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://infomall.org/pubs/content.html&#34;&gt;List of Publication files&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/presentations/&#34;&gt;Presentations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/memories/&#34;&gt;List of files in Memories&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;upload-of-new-documents&#34;&gt;Upload of new documents&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The document archive is located on rivanna. For papers they need to
be uploaded to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;/project/bii_dsc/www/infomall/publications&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For presentations, they need to be uploaded to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;/project/bii_dsc/www/infomall/presentations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After the upload the paper needs to be added to
&lt;a href=&#34;https://docs.google.com/document/d/1M61ieHlwHs96ZL3fZCtQ_zAbgH69vnPWn2S6C1eCGqY&#34;&gt;google drive paper list at&lt;/a&gt;
A presentation need to be added to &lt;a href=&#34;https://docs.google.com/document/d/1KaJTAwEOI7fnJof24V5awyLpKhxTp7xDRoidBsOPG3M&#34;&gt;google drive presentation list&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After the papers have been uploaded, Gregor von Laszewski will
update the html page that si generated while downloading the paper
and html pages as html files from google.
The resulting html page needs to be renamed to index.html and
uploaded into the directory on rivanna that hod the presentation
and publication directories.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deprecated&#34;&gt;Deprecated&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Publications on the Static archive site&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://archive-infomall.org/publications/&#34;&gt;Publications Archive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://archive-infomall.org/presentations/&#34;&gt;Presentations Archive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://archive-infomall.org/memories/&#34;&gt;Memories archive&lt;/a&gt; is now
contained in &lt;a href=&#34;https://infomall.org/memories/&#34;&gt;Memories&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/pubs/index-cgl.html&#34;&gt;Community Grids Lab 2015&lt;/a&gt;
(till 2015)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/pubs/index-Actual.html&#34;&gt;Community Grids Lab 2012&lt;/a&gt;
(till 2012)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/pubs/index-old.html&#34;&gt;Community Grids Lab 2012&lt;/a&gt;
(till 2012)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/pubs/index1.html&#34;&gt;Community Grids Lab 2005 1&lt;/a&gt;
(till 2005)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/pubs/index1.html&#34;&gt;Community Grids Lab 2005 3&lt;/a&gt; (till 2005)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/pubs/index2.html&#34;&gt;Review and Resource Material from FSU Information Technology Research Group 2000-2001&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://infomall.org/pubs/disted.html&#34;&gt;Distance education&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://infomall.org/presentations/index-2015.html&#34;&gt;Presentations 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/pubs/presentations.1.html&#34;&gt;Presentations 2012&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://infomall.org/pubs/content.html&#34;&gt;Publications Dir Content&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Sites</title>
      <link>https://infomall.org/uva/docs/publications/sites/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://infomall.org/uva/docs/publications/sites/</guid>
      <description>
        
        
        

&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;This page contains links to sites that have been backed up or are still active.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;The sites have been backed up with a static site duplicator or have been copied from its original source. As some Web sites wer generated with some specialized software or relied on databases not everything is working. We have tried to preserve most of the information.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://archive-infomall.org/www.dsc.soic.indiana.edu/&#34;&gt;DSC at IU static archive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;static-web-site-archives&#34;&gt;Static Web Site Archives&lt;/h1&gt;
&lt;p&gt;For more static Web site Archives please see
&lt;a href=&#34;https://infomall.org/archive/&#34;&gt;Infomall Static Web Site Archive&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
